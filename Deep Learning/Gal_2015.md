# Title: Dropout as a Bayesian Approximation: Insights and Applications (2015)

# Author: Gal & Ghahramani

#### General Content: 

Introduction of one of the first Bayesian frameworks for Deep Learning. GP perspective allows to formalize the uncertainty over parameters. One does not need to compute the exact GP but instead is able to approximate the covariance function via Monte Carlo integration.

#### Keypoints: 

Authors show that MLP with dropout at each layer is equivalent (in terms of objective function) to Approximate Variational Inference for Deep GPs. The analogy to Gaussian process regression is a possible explanation for why Deep NNs with dropout generalize well. Furthermore, we are able reason about uncertainty over the complicated exacted features. Authors suggest that one has to put an approx variational distribution over the bias vector and not only the weights.

#### Summary:

* Dropout seems to be related to SGD on L^2-penalized error function.
* Show that dropout objective minimizes KL divergence between approx. model and deep Gaussian process.
* Important to readjust after dropout layer by rescaling the weights by inverse dropout probability
* View MLP as one deeply composed function: In GP setup non-stationary covariance functions/kernels correspond to hyperbolic or ReLU MLPs. -> indirect assumption on the smoothness of the function
* Don't perform the full GP, which would take a O(n^3) matrix inversion. Instead one approximates the full/true GP to obtain a managable time complexity. 
* Minimising the KL div <-> maximising the log evidence lower bound. Maximisation leads to variational distribution which explains the data well while still not deviating too much from the prior distribution.
* Approximate the exact covariance function by a MC integration. The paper obtains an approximate objective function for this "deep GP NN version" and shows equivalence of the objective functions.
* One can't evaluate the KL div term between a mixture of Gaussians and a single Gaussian analytically!
* Common knowledge: doubling the learning rate of the bias during MLP optimisation works very well in practice with dropout nets with p=0.5
* MC dropout: Estimate the dropout model using T forward passes through the net and averaging the results - model averaging.

#### Questions:

* How does the rescaling work? - only at test time and not at training
* How does one optimise the dropout proportion?
* What does model assumption (section 2.3) of assuming one latent set of variables, which act as sufficient statistics, affect the model flexibility/generative model? Common assumption for VI?

#### Other:

* Stationary Kernel: Function whose length scale does not change throughout the input space will be well modeled by a GP with a stationary kernel - can just be written as x - x'


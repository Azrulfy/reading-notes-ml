# Title: On Modern Deep Learning and Variational Inference

# Author: Gal & Ghahramani (2016) 


#### General Content: 
Modern deep learning models can be formulated as performing approx. variational inference in a Bayesian setting. Paper extends Gal & Ghahramani (2015), which showed that a MLP with dropout at every layer corresponds to approx. VI in Deep GPs, to a mor general Bayesian NN/deep learning models setting. Furthermore, they show that any form of stochastic regularization in arbitrary NN structures can be viewed as approx. variational inference in Bayesian Nets. Ultimate goal: Synthesize world of stats and VI with deep learning in order to form a mathematical grounding


#### Keypoints: 
 
* Show that **any form of stochastic regularization in arbitrary NN structures can be viewed as approx. variational inference in Bayesian Nets: Almost all deep learning models perform approx. Bayesian inference to capture the latent stochastic process underlying the data.
	* Allows us to reason about the full DL framework in a Bayesian manner: assess model uncertainty, model comparison and uncertainty over features/inference. 	

#### Summary:

* Want to be able to reason about parameter and *structural* uncertainty of neural nets.
* **Multivariate Gaussian noise**: alternative to dropout. Does not sample bernoullis but N(1,1) and multiplies this random sample with the weights. Slows training down more than bernoullis because harder to sample from the Gaussian.
* **Methaphorical Interpretation of dropout**: Sexual reproduction - many sperm cells are "send" and only one is needed to impragnate the women = Unnecessary redundancy in the data. Cannot be proven since Gaussian noise (which are non-exactly-0 draws) works similarly well.
* CNNs only work when dropout is only used for the non-convoluted layers. Possible reason: less co-adaption in convolution layers, already sparse -> no dropout needed. 

* Here: Derivation focuses on multiclass-classification problem and they derive an intractable GP posterior which afterwards is approximated by introducing a variational distribution over the latent variables.
* Minimization of KL div between variational distribution and parameter posterior is equivalent to maximising the log evidence lower bound.
* Gal & Turner (2015): Show that one is able to approximate the exact GP by defining an approx. distr. over spectral frequencies and their coefficients in a Fourier decomposition if the function.

* **Structural Formulations**:
	* Different nonlinearity-regularisation techniques correspond to different GP kernels, which have different impact on the predictive uncertainty! 
	* Multiple variables: Apply kernel the concatenated matrix of variables
	* Two seperate activations for two different variables: Summing of two Kernels

* **Bayesian Neural Nets**:
	* Plaxe a prior distr. over the weights - often matrix Gaussian prior
	* Interested in finding most likely weights given the data -> use VI to compute approx. to otherwise intractable integral.
	* Use Monte Carlo integration to approximate the approximation of the KL divergence
		* Suggests that stochastic regularisation techniques work well since they approximately integrate over the model parameters!
	* **MC dropout**: Make predictions using the predictive distr. approximation with VI-obtained MC integrated posterior! 	
	

#### Questions: 

* What are spectral frequencies? 
* It should be easily possible to find/create new application specific stochastic regularisation variants. Enforce correlations between the weights.
* Extract Bayesian inference knowledge about the GP - how can we learn something from NNs?

# Title: Policy Distillation

# Author: Rusu et al (2016)

#### General Content: Method to extract policy of RL agent and to train new net that performs at expert level while being smaller and more efficient. Furthermore, combines multiple task-specific policies into single one and is able to online continuously distill best policy - efficient way to track evolving Q-learning policy.

#### Keypoints: 

* Distillation: Efficient means for supervised model comparison
	* Creation of single net from ensemble model
	* Optimization method that stabilizes learning
	* Uses supervised regression to train target net to produce same output distribution as original net often using less peaked/softened target distribution
	* policy version - proves that supervised learning can generalize to sequential prediciton tasks
	* Increase of temperature of softmax: transfer more knowledge from teacher to student

* Relation between Distillation - imitation learning - multi-task learning 	
* Teacher model T, student model S - teacher outputs softened softmax of vector of q-value vector
	* Student learns this distribution with the help of regression/different net
	* 3 different loss functions: 
		* negative log LH (NLL) - preserve greedy action of teacher
		* MSE - preserve full set of action-values in student model
		* KL divergence
	* Experiments: KL performs best - agent often even outperforms the DQN teacher!
		* MSE: Bad. Greedy action choices can be made based on very small differences in Q-values - receive low weight in MSE
		* NLL: Assumes that single aciton choice is correct at any point in time. If teacher not perfect, this amplifies noise!
		* KL finds balance between two - softening param around 0.01  

* Classification version of distillation: output distribution q^T usually very peaked - good to soften
	* Policy distillation: outputs of teacher not distribution but rather exp future disc reward of each possible action
	* Rather than soften, we want to make them sharper!    

* Multi-task distillation: Since different tasks often have different action sets, we need different output layer trained for each task
* Policy distillation for model compression: Likely that final policy does not require all of capacity of DQN
	* Can even outperform - form of regularization
	* Training larger net can accelerate policy iteration cycle of DQN - perform distillation after full training
* Online policy distillation: student must track teacher during training - distilled agent much more stable!  
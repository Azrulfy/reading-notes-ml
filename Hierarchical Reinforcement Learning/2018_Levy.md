# Title: Hierarchical Reinforcement Learning with Hindsight

# Author: Levy et al (2018)

#### General Content: Authors propose to merge UVFA, HER and hierarchies of policies. Each hierarchical level proposes subgoals for the level below. Hindsight is then used to solve all MDPs within hierarchy simultaneously.

* 2 different ways to use hindsight:
	1. Agent replays actions with different goals: learn to generalize to unseen goals
	2. Agents replays higher-level decisions using subgoal states achieved in hindsight as the subgoal actions. This provides way for agent to discover high-level subgoal actions belonging to a particular time scale autonomously. Furthermore, leads to sample efficiency - allows agent to evaluate higher level actions even when lower level layer has not fully learned to achieve higher level subgoals.


#### Keypoints: 
* Universal MDP: $U(S, A, T, R, G)$ where $R: S \times A \times G \to \mathcal{R}$ or just $R(s,a,g)$
* Solve $U_{original}$ via learning hierarchical policy that solves hierarchy of k UMDPs $U_0, ..., U_{k-1}$ where each UMDP represents a different level of temporal abstraction
	* $U_0$: lowest level of hierarchy - $S_0 = S$, $A_0 = A$, $G_0 = S_0 =S$
	* $U_i, 0<i<k-1$: $S_i = S$, $A_i = G_{i-1}$, $G_i = S_i$ 
	* $U_{k-1}: $S_{k-1} = S$, $A_{k-1} = S$, $G_{k-1} = G$
* Action Selection - top-down
	* $s \in S, g \in S$  are initialized $\to$ $\pi_i: S_i \times G_i \to A_i$ yields $a^{k-1}$
	* $a^{k-1}$ initializes $g_{k-2} \in G_{k-1}$ and $\pi_{k-2}(s, g_{k-2} = a^{k-1}$ yields $a^{k-2}$
	* Recursion all the way to bottom $\pi_0(s, g_0=a^1)$
	* Note: upper layer Action knits lower level goal!
* Action Execution - bottom-up
	* $a^0$ selected - $\pi^0$ continues until $s=g_0$, afterwards return control to next higher level.

* Motivation for hindsight: In beginning of training we have $a^i = g^{i-1}$ but $\pi_{i-1}$ might not know how to achieve, e.g. $s_{t+1} \neq a^i = g^{i-1}$
	* $(s_t^i, s_{t+1}^i, g_t^i, s_{t+1)^i, r_{t+1}^i)$: Transition if subgoal achievement failed - action replay: process of substituting hindsight subgoal state for action
	*  $(s_t^i, a_t^i, g_t^i, a_t^i, r_{t+1}^i)$: Transition if subgoal achievement succeeded
	*  $(s_t^i, a_t^i, g_t^i, s_{t+1)^i, r_{t+1}^i)$: standard form of a transition

* Action Replay:
	* Learn with suboptimal policies
	* Helps high level agent to discover new subgoal actions that satisfy appropriate time scale d
	* Helps high level agents to eval subgoal actions even when lower level layers have not fully learned to achieve those subgoal actions

* Goal Replay:
	* Generate set of additional experiences $\{(s_t^i, a_t^i, g^i, s_{t+1}^i, r_{t+1}^i | g^i \in G^i\}$ where G denotes set of additional goal configurations with which we will augment training data
	* Enables learning with enough experience

* Tabular Discrete Version - Hierarchical Q-Learning
	* Augment with complete goal space in goal replay 
	* Each layer learns at frequency of time scale of its actions
	* Lower level policy learns after each low-level action using goal replay
	* Higher level policy learns using action replay every d actions

* Continuous Fct Approx Version - Hierarchical AC
	* Fct approx + off-policy AC RL also such as DDPG
	* Subgoal testing: for % of time, no noise is added to any of the policies during training
		* If level proposes subgoal that is not achieved then penalize with low reward
		* Prevent Fct approx from producing unrealistic subgoals over and over
		* If proposed subgoal is never reached Q-Val for subgoal action may never be updated
		* Divergent info - authors think that these washout  
	

#### Questions: 
* User has to specify d the time-scale/max number of attempts to achieve the subgoal - how to do? Domain-knowledge again?
* Each layer learns form both action and goal replay in hierarchical learning
* Others talk about separate training of layers as in Feudal but mention that each manager in FUN is responsible for a separate part of the space which leads to questions in continuous spaces - why donâ€™t we have this problem here as well? - propose to combine hindsight with FUN
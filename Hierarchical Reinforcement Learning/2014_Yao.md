# Title: Universal Option Models

# Author: Yao et al (2014)

#### General Content: Authors deal with the problem of learning models for options in real time and in the setting where reward functions can be specified at any time and expected returns must be efficiently computed.

	* UOM: option model independent of any reward fct - universal wrt rewards
	* Show how to extend notion to linear fct approx
	* Show that UOM gives TD solution of option returns - proof convergence


#### Keypoints: 

* Classic option models cant deal with multiple-reward planning problems. When reward fct is changed abstract planning with traditional option model has to start from scratch

* UOM consists of 2 parts:
	1. State prediction: predict state of option termination
	2. Accumulation: predict occupancies of all states by option after execution starts - similar to Dayan's successor representation

* Option model: For option $o$: $<R^o, p^o>$
	* $R^o$ - option return: total exp disc return until option terminates (stochastic waiting time T)

	$$R^o(s) = \mathbb{E}[r_1+ \gamma r_2 + ... + \gamma^{T-1} r_T]$$
	
	*   $p^o$ - discounted terminal distribution: disc prob of termination at $s'$ after option is initiated in $s$

	$$p^o(s, s') = \mathbb{E}_{s,o}[\gamma^T \mathbb{1}_{\{s_k =s'\}}] = \sum_{k=1}^\infty \gamma^k \mathbb{P}_{s,o} \{s_T = s', T=k\}$$

* Universal option model: $<u^o, p^o>$ where $u^o$ is option's discounted state occupancy function.

$$u^o(s, s') =\mathbb{E}_{s,o}[ \sum_{k=0}^{T-1} \gamma^k \mathbb{1}_{\{s_k =s'\}}]$$

and 

$$R^o(s) = \sum_{s' \in S} r^\pi(s') u^o(s,s')$$

* Theorem: Traditional option model can be constructed from UOM and reward vector of option. Vector essentially conditions universal model to specific option.

* Authors show that to compute TD approc of option return corresponding to R it suffices to find LS approx of exp one-step reward under the option and R provided one is given U matrix of option.

* TD(0)-based linear UOM - reward independence!
* Construct multiple immediate reward models for different reward signals of interest
* Compare to LOEM (linear option exp model) by Sorg and Singh (2010) - estimation from experience
	* Learning LOEM model is faster (computing time) then learning UOM for single fixed reward function
	* UOM can produce accurate option return quickly for each new reward function


#### Questions: 

* Read up on Dyna and more model-based RL!
* Authors use computer time to compare approaches!!! very code dependent very bad!!!
# Title: A Probabilistic Earley Parser as a Psycholinguistic Model

# Author:  John Hale (2001)

#### General Content: The cognitive load of language comprehension can be expressed as the total prob of structural options that have been disconfirmed at the point in sentence - paths in tree that have been traveled unsuccessfully. This again be formulated as suprisal of a word given its precessors. Measure can be efficiently calculated using a probabilistic version of the Earley parser from a probabilistic phrase-structure grammar. Is able to account for Garden Path sentences.


#### Keypoints: 

* Garden Path Sentence: Grammatically correct sentence that starts in way that reader's most likely interpretation will be incorrect. Reader deceived into dead end parse. Sentence creates temporary ambiguity/multiple interpretations.
* Relationship knowledge of gramar - application in perceiving syntactic structure - 3 principles:
	1. Parser - Grammar: Strong competence - parser uses rules of grammar
	2. Frequency affects performance - stat theory of language performance
	3. Eager sentence processing: unrushed experimental setup

* Stolcke's prob Earley parser: Way to use hierarchical phrase structure in language model such as ngram
	* Prob CFG: sentence prob - product of all rules used to generate
		* multiplication - assumption that rule choices are indep
		* consistency: does grammar assign non-0 prob to inf recursion?
		* problem: only assigns probs to complete sentences of language
	* Compute prefix prob at each word - sum of probs of all derivs whose yield is compatible with string seen so far
	* 1 - prefixprob = prob of all other derivs that have been disconfirmed (given consistency)

* Earley parser: top-down - propagation of prediction back up through set of states of states representing hypotheses parser is using about struxture of the sentence.
	* collection of states - chart - tree set
	* state:
		* current input string position processed so far
		* grammar rule
		* dot-position in rule: how much of rule recognized
		* left-most edge of substring rule generates
	* functions:
		* predict, scan, complete
		* S $\to$ predict (add new states for prod. rule following S) $\to$ scan (check input string. If match move dot) $\to$ complete (propagate change through graph)
		* Earley paths <-> correspondence with derivations
	* Stolcke: add two infos to each state: $\alpha$ - prefix prob and $\gamma$ - inside probability
	* Earley path: sequence of Earley states linked by three ops
	* State completion: bottom-up confirmation - $\alpha$ unites with top-down prediction $\gamma$          

* Total parallelism theory: Entire set of trees compatible with input is maintained somehow from word-to-word

* Cognitive effort expanded to parse prefix - prop. - total prob of all structural analyses which cant be compatible with observed prefix
	* generate predictions about word-by-word reading times by comparing total effort expended before some word to the total effort after
	* assumption: probs of PCFG rules = statements of how difficult it is to disconfirm each rule
	* $log(\frac{\alpha_{n-1}}{\alpha_n})$ - surprisal: combined difficulty of disconfirming all disconfirmable structures at a given word

* Garden paths are points where the parser can disconfirm alternatives that together comprise a great amount of probability
	* effect disappears when words intervene that cancel reduced relation interpretation early on
	* Evidence for total-parallelism parsing theory	  

#### Questions: 

* How to incorporate this into option/substructure discovery problem? - options over options - specific attention to beginning
* Info-theoretic based grammar extraction - IGGI
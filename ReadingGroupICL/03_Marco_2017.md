# Title: Virtual vs. Real: Trading Off Simulations and Physical Experiments in Reinforcement Learning with Bayesian Optimization

# Author: Marco et al (2017)

#### General Content:
Authors derive a Bayesian Optimization scheme (namely entropy search) for real-life RL settings in which experiments are associated with costs. The framework trades off the different costs associated with real-life and computer simulation experiments. It is named "multifidelity entropy-search".


#### Keypoints: 

* Entropy search: Maximise information gain for each query/experiment
	* Select paramters which maximally reduce the uncertainty about the uncertainty about location of minimizer - make minimizer distribution collapse at minimizer, which means low entropy distribution! $\mathbb{P}(\theta \in argmin_{\theta} J(\theta))$ 
	* Use non-uniform grid with more points/higher resolution in areas where it is "more likely" to find the minimum. 
	* What does this mean? Why only evaluate for already strong belief points? Don't we want to have more exploration? <-> Relationship to expected improvement (EI)
* Proposed framework generalizes this to case of multiple info sources $\to$ way of efficiently combining cheap, but inaccurate info from simulations with expensive but inaccurate info from real physical experiments
* Introduce conisistent unit of information gain: $\mathbb{E}[\Delta H_i(\theta)]/t_i$ where $t_i$ is some form physical meaningful cost, $i \in \{sim, exp\}$.


#### Summary:

* Approach is different from *2-stage learning*: System automatically chooses environment in which we run the experiment.
* Transfer learning - similarly use some form of prior knowledge - two distinct steps - used to generalize between different tasjs
* Policy gradients - RL technique: Policy parameters are improved locally along the gradient
* Multi-task Bayesian Optimization (MTBO): Try to transfer knowledge about two related tasks
	* Very very similar - Switched kernel to be additive and not multiplicative! 
* Previous approaches: No explicit modelling of different experiment costs
* Simulator comes usually for free! <-> by-product of robot design

* Def.: Costs - costs resulting from lack of accuracy
* Def.: Effort - costs resulting from retrieving an evaluation from the system

* Kernel - encodes smoothness assumption and rate of change assumptions of unknown target function

* $a = (\theta, \delta) \to k(a, a') = k_{sim} (\theta, \theta') + k_\delta (\delta, \delta')k_{err}(\theta, \theta')$
* $k_\delta (\delta, \delta') = \delta \delta'$ - 1 if both parameters indicate physical experiment - $\delta = 0$: indicates simulation
* Kernel specification: two experiments on physical system covary strongly - error covariance turned off in simulations in order to model that sims can't provide all info about the function
* Incorporated different noise variances for measurements - This plus the kernel structure allows to incorporate multiple information sources into single GP model


#### Questions: 

* How does this generalize to non-linear systems? Very simple optimization structure used in example experiment!

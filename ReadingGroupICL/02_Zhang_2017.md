# Title: Theory of Deep Learning III: Generalization Properties of SGD

# Author: Zhang et al (2017)

#### General Content: 

Authors analyze the role of SGD for the generalization property of hugely overparametrized NNs. They argue that SGD does not only minimize the loss function but also maximizes the "degeneracy" of the obtained optimizers. I.e. SGD finds minima with large flat regions/volume. 

#### Keypoints: 

This essentially leads to input robustness since perturbing the weights can be viewed as equivalently varying the inputs! They base their results on three main observations:

* SGD can be written as Langevin dynamics which converge to a Boltzmann distribution whose mode can be found in the flat minima
* Flat regions can be characterized by a hyperplane. Finding this hyperplane corresponds to a LS problem which has to be solved using a pseudoinverse. Pseudoinverses regularize indirectly - see Tikhanov regularization.  
* Connection to robust optimization - perform implicit regularization
* Consistency may hold in the absence of generalization!
* Good accuracy does not imply good generalization

* SGD minimizes empirical loss but also maximizes the volume/flatness of the minima -> effectively linearizes the problem wrt the weights
* In linear case SGD and GD converge to pseudoinvers-base solution

* Langevin analysis observation: SGD selects with high prob large volume minima (mostly correspond to flat minima), because those have large regions of zero loss
* For flat minima SGD/GD converge to pseudoinverse solutions <-> solution found is minimum L1 norm among the infinite degenerate solutions that correspond to zero empirical error.
* Minimum norm solutions <-> large margin solutions
* Overparametrization does not necessarily improve test error which is otpimal around boundary between over- and under-parametrization.
* Perturbations in weights and inputs - Exactly what generalization means!!! - relationship between different forms of regularisation

* **Explanation for two puzzles**
	* DL does not generalize for randomly labeled training data, but does so for natural labels - Reason: Larger margin and larger volume regions can be found for natural labels. Data generating process induces exploitable pattern
	* There occurs no overfitting with increasing network size - Training set size is more important than the number of parameters 

* Fast convergence implicitly indicates that SGD has a large basin of attraction -> strong flat regions which imply good generalization performance!

#### Summary:

* Generalization: Difference between empirical risk and expected error goes to zero as the training sample size increases.
* Consistency: Empirical error converges to optimal Bayes' risk with increasing training sample size.
* Classical Approaches of statistical learning theory to NNs: VC dimension (restrict complexity of hypothesis space); Large Margin Classifiers (Min Norm <-> Large Margin in linear case)
* Keskar et al: Large batch sizes tend to converge to sharp minimizers and bad generalization behavior
* 2 Ways to prove convergence of SGD:
	1. Partition parameter space into several attraction basins - assume that after few iterations algo confines parameters in single basin - proceed as in convex case
	2. Instead of showing that function f converges (dynamical systems approach), show that cost function and its gradient converge.  
* Pseudoconvex functions: Function that behaves like a convex function wrt finding its local minima, but need not actually be convex. A differentiable fct is pseudoconvex if it is increasing in any direction where it has a positive directional derivative

* **SGD as approximate Langevin Equation**
	* Vector of gradient uodates has an approximate Gaussian distribution -> follows from CLT
	* Write SGD as DS with noisy true risk -> *discretized Langevin Diffusion*
	* If noise is derivative of Brownian Motion it is a Langevin equation -> Stochastic Dynamical System
	* Has an associated *Fokker-Planck equation* on probability distribution of f
	* Asymptotic distribution is *Boltzman distribution* -> puts more weight on degenerate optima
	* Discretized version of Langevin dynamics is equivalent to MH for small eta 

#### Questions: 

* Need to learn more about statistical dynamics and the meaning of the Boltzmann distribution <-> also: Thermodynamics and FEP connection!

* Learn more about robust optimization.

* Learn more about discretized Langevin Diffusion

* How does this relate to finding an optimal mini-batch size?

* What are conditions on data generating process that make performance of DL strong?
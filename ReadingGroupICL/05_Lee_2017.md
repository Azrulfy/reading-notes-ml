# Title: Deep Neural Networks as Gaussian Processes

# Author: Lee et al (2017) 

#### General Content: 

Shows how to recursively model the covariance function of a Gaussian Processes to obtain exact Bayesian Inference in Deep NNs without stochastic regularization. They derive correspondence of the function representation and develop computational efficient pipeline for computation using a look-up table. Furthermore, they show that using this approach for standard classification tasks one can show that there is a strong correlation between the GP uncertainty and prediction error. Allows us to apply full Bayesian framework - model uncertainty and perform model comparison.

#### Keypoints: 

* Neal (1994a) - Connects deep NNs in the limit with finite width with GPs
* Duvenaud et al (2014) - Deep GPs -> degenerate form of kernels that are infinitely times composed.
* Cho & Saul (2009) - derive compositional kernels for ReLU
* Lawrence and Moore (2007) - first DGP paper? Hierarchical GPs
* Weight and biases are drawn from Gaussian prior - infinite width (hidden units) allows us to apply CLT - function computed by the NN is a draw from a specific GP
* iid prior over weights and biases is equivalent to specific GP prior over functions
* Expectation over Gaussian input to ReLU is Gaussian again - integral over 0 to infinity though!
* Hidden units pre-activation are drawn from GP -> by induction this holds for the next layers as well. 
* Obtain one single kernel expression for all hidden units (since they are iid! - watch out for notation!) - Integral becomes deterministic function
* Single layer infinite width network is a GP (known from Neal) <-> Deep network as well GP --- Hence a deep network is basically the same as a shallow but wide network
* Weakness of GP in limit - loss of correlation - not the case for the deep net
* Deep Signal Propagation - Recurrence relations lead to ordered/chaotic phase:
	* Ordered - recursion similarises dissimilar inputs: Constant kernel value
	* Unordered - weight variance dominates: inputs become dissimilar from random matrix projections
* Bounded/Unbounded phase:
	* Can only learn something in specific hyperparameter domain.
* GPs as alternative to standard SGD trained NNs

#### Questions: 

* What is the direct contribution? - Approximation through lookup table and Bayesian Inference
* Generalized Cho & Saul (2009) to tanh. Otherwise mainly reproduction.
